{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5cfa975-135a-40a1-9e14-059e669aa588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b330d94-cef8-4f52-867a-28543b1a1fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"abfs://data@datalakesii.dfs.core.windows.net/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04a4d5b6-58a9-46d8-af63-4a070d2f7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         namespace|\n",
      "+------------------+\n",
      "|          apa_temp|\n",
      "|           default|\n",
      "|                dw|\n",
      "|          dwbgdata|\n",
      "|information_schema|\n",
      "|            libsas|\n",
      "|               sdj|\n",
      "|               sys|\n",
      "|               tmp|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "#CONNECTION_NAME = \"datalakesii-dev\"\n",
    "#conn = cmldata.get_connection(CONNECTION_NAME)#\n",
    "#spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9201356-d5ff-46f2-b9a3-17ac78985daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|emp_id|    name|emp_dept_id|\n",
      "+------+--------+-----------+\n",
      "|     3|Williams|         10|\n",
      "|     4|   Jones|         30|\n",
      "|     1|   Smith|         10|\n",
      "|     2|    Rose|         20|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_SQL_QUERY = \"select * from apa_temp.empdata\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817bcb36-b958-4b5f-9fb4-07a7b8a1476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empdataDF=spark.sql(EXAMPLE_SQL_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7379bef-7b59-4497-aa3c-a1c0f7427b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(empdataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d9c3f2-637c-4ea1-9d63-229be260cad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|emp_id|    name|emp_dept_id|\n",
      "+------+--------+-----------+\n",
      "|     3|Williams|         10|\n",
      "|     4|   Jones|         30|\n",
      "|     1|   Smith|         10|\n",
      "|     2|    Rose|         20|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06fce546-8220-40f2-a183-6d477175864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 18:47:21 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|\n",
      "+------+--------+-----------+---------+-------+\n",
      "|     3|Williams|         10|  Finance|     10|\n",
      "|     4|   Jones|         30|    Sales|     30|\n",
      "|     1|   Smith|         10|  Finance|     10|\n",
      "|     2|    Rose|         20|Marketing|     20|\n",
      "+------+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_SQL_QUERY = \"select * from  apa_temp.empdata e inner join apa_temp.deptdata d on ( e.emp_dept_id == d.dept_id)\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a33e5-5120-4097-8296-2223fc90971e",
   "metadata": {},
   "source": [
    "**lo rescato como DF y lo guardo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c9e1cd5-2021-493f-b036-9b272f20f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruceSimple=spark.sql(EXAMPLE_SQL_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46d00d5-ea23-4ecc-9145-eef0e0aa2b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 18:47:22 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|\n",
      "+------+--------+-----------+---------+-------+\n",
      "|     3|Williams|         10|  Finance|     10|\n",
      "|     4|   Jones|         30|    Sales|     30|\n",
      "|     1|   Smith|         10|  Finance|     10|\n",
      "|     2|    Rose|         20|Marketing|     20|\n",
      "+------+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruceSimple.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa762782-d341-4ba5-ae81-e2b7bb1768fe",
   "metadata": {},
   "source": [
    "**acabo de hacer un cruce simple en spark, si deseo usar alguna librería clasica de ML podría querer pasar este DF de spark a pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b968cbf-0b09-40ee-9a9e-5fc295dde49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cruceSimple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2facd9f-f235-44b2-9cb0-33869cc1ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 18:47:22 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    }
   ],
   "source": [
    "cruceSimplePandas=cruceSimple.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d05066c-c3fd-4bb1-9914-6543d020a487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cruceSimplePandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55bbd746-7e96-439c-b00a-84f57448ef5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp_id</th>\n",
       "      <th>name</th>\n",
       "      <th>emp_dept_id</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>dept_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Williams</td>\n",
       "      <td>10</td>\n",
       "      <td>Finance</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Jones</td>\n",
       "      <td>30</td>\n",
       "      <td>Sales</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Smith</td>\n",
       "      <td>10</td>\n",
       "      <td>Finance</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Rose</td>\n",
       "      <td>20</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emp_id      name  emp_dept_id  dept_name  dept_id\n",
       "0       3  Williams           10    Finance       10\n",
       "1       4     Jones           30      Sales       30\n",
       "2       1     Smith           10    Finance       10\n",
       "3       2      Rose           20  Marketing       20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cruceSimplePandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa6f59-1be9-450b-ba15-d9d9a44e99cf",
   "metadata": {},
   "source": [
    "**podria querer guardar el archivo pandas como df spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a60e9440-92f1-40e6-847e-56bffb2f6ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- emp_dept_id: long (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|\n",
      "+------+--------+-----------+---------+-------+\n",
      "|     3|Williams|         10|  Finance|     10|\n",
      "|     4|   Jones|         30|    Sales|     30|\n",
      "|     1|   Smith|         10|  Finance|     10|\n",
      "|     2|    Rose|         20|Marketing|     20|\n",
      "+------+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "cruceSimplePandas.iteritems = cruceSimplePandas.items\n",
    "cruceSimplePandasSpark=spark.createDataFrame(cruceSimplePandas) \n",
    "cruceSimplePandasSpark.printSchema()\n",
    "cruceSimplePandasSpark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a18f6-320e-49e2-a8cb-a3fa94e21379",
   "metadata": {},
   "source": [
    "**ahora como spark lo puedo guardar otra vez**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3cd091-ae80-4b4c-85f2-dca75dca9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "project=os.environ[\"CDSW_PROJECT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e65adb-edbd-47c4-8fae-2508dff64264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prueba_hive_tablas/tablas\n"
     ]
    }
   ],
   "source": [
    "rutaPersonal=project+'/tablas'\n",
    "print(rutaPersonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f8ef88a-6555-4f7f-a313-189065fd1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/02 18:47:26 WARN ShellBasedUnixGroupsMapping: unable to return groups for user manuel.barrientos\n",
      "PartialGroupNameException The user name 'manuel.barrientos' is not found. id: ‘manuel.barrientos’: no such user\n",
      "id: ‘manuel.barrientos’: no such user\n",
      "\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:291)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:215)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroupsSet(ShellBasedUnixGroupsMapping.java:123)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupSet(Groups.java:413)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:351)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:300)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3953)\n",
      "\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3976)\n",
      "\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4960)\n",
      "\tat org.apache.hadoop.security.Groups.getGroupInternal(Groups.java:258)\n",
      "\tat org.apache.hadoop.security.Groups.getGroupsSet(Groups.java:230)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupsSet(UserGroupInformation.java:1761)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1727)\n",
      "\tat org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:64)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:408)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:1004)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1788)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1388)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:230)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:157)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1138)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:607)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:331)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:258)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:257)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:311)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:605)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:510)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:399)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:274)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:100)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:380)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:671)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/08/02 18:47:26 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/08/02 18:47:26 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n"
     ]
    }
   ],
   "source": [
    "cruceSimplePandasSpark.write.mode('overwrite').option(\"path\", \"abfs://data@datalakesii.dfs.core.windows.net/DatosProcesados/%s/\"%(rutaPersonal)+\"cruceSimple\").saveAsTable(\"apa_temp.cruceSimplePandasSpark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815c241-81e1-4419-9252-e8db38b93ef2",
   "metadata": {},
   "source": [
    "**ahora podria querer leer la tabla que acabo de guardar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92c74ae9-6d7a-4e41-b91a-05f4f7fa4c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|\n",
      "+------+--------+-----------+---------+-------+\n",
      "|     3|Williams|         10|  Finance|     10|\n",
      "|     4|   Jones|         30|    Sales|     30|\n",
      "|     1|   Smith|         10|  Finance|     10|\n",
      "|     2|    Rose|         20|Marketing|     20|\n",
      "+------+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_SQL_QUERY = \"select * from  apa_temp.cruceSimplePandasSpark\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8a584-66ba-49cb-964c-d2f0dc429aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929819a-d199-4c7c-a1d0-5ba7b0a14a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f4458-9323-4170-b6ba-cb96354e1edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
